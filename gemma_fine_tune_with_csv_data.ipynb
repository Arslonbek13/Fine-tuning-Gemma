{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"cell_execution_strategy":"setup","gpuType":"T4","mount_file_id":"1-Wm-NQNW0_CWCrXTzN_9oDCsDKHJV220","authorship_tag":"ABX9TyPblNfK7Ps6/qlDzTTNm5Or"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"habX9FaYOc3A"},"outputs":[],"source":["!pip install kagglehub\n","import kagglehub\n","kagglehub.login()"]},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","os.environ[\"username\"] = userdata.get('username')\n","os.environ[\"key\"] = userdata.get('key')"],"metadata":{"id":"_hn_d1j4Ow7G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q -U keras-nlp\n","!pip install -U keras"],"metadata":{"id":"_WipUTXwO4HW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"KERAS_BACKEND\"] = \"jax\"\n","os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""],"metadata":{"id":"bN64G5bTO9Po"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","\n","file_path = 'combined.csv'\n","df = pd.read_csv(file_path)\n","\n","\n","print(df.head())"],"metadata":{"id":"jJ0UIFicPQhg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["instruction_col = 'prompt'\n","response_col = 'response'\n","\n","data = []\n","template = \"Instruction:\\n{instruction}\\n\\nResponse:\\n{response}\"\n","\n","for _, row in df.iterrows():\n","    data.append(template.format(instruction=row[instruction_col], response=row[response_col]))\n","\n","\n","data = data[:1000]"],"metadata":{"id":"l5u2PkwnPmgg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[10]"],"metadata":{"id":"EbQOeoGuPpKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"LoqMFEBPpwWm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keras\n","import keras_nlp\n","\n","# Load the GemmaCausalLM model\n","gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n","gemma_lm.summary()\n","\n","# Enable LoRA on the model\n","gemma_lm.backbone.enable_lora(rank=4)\n","gemma_lm.summary()\n","\n","# Set the sequence length for the preprocessor\n","gemma_lm.preprocessor.sequence_length = 512\n","\n","# Initialize the optimizer with AdamW\n","optimizer = keras.optimizers.AdamW(\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n",")\n","\n","# Exclude certain variables from weight decay\n","optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n","\n","# Compile the model with loss and metrics\n","gemma_lm.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=optimizer,\n","    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",")\n","\n","# Assuming `data` is a preprocessed dataset\n","# Fit the model to the data\n","gemma_lm.fit(data, epochs=1, batch_size=1)\n","\n","# Save the model to Google Drive in native Keras format\n","model_path = '/content/drive/MyDrive/Gemma/gemma_lm_model.keras'\n","gemma_lm.save(model_path)\n"],"metadata":{"id":"srxJVQVmp1xM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"qsNW0uAEp_Ox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import keras\n","import keras_nlp\n","\n","\n","gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n","gemma_lm.summary()\n","\n","\n","gemma_lm.backbone.enable_lora(rank=4)\n","gemma_lm.summary()\n","\n","\n","gemma_lm.preprocessor.sequence_length = 512\n","\n","\n","optimizer = keras.optimizers.AdamW(\n","    learning_rate=5e-5,\n","    weight_decay=0.01,\n",")\n","\n","\n","optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n","\n","\n","gemma_lm.compile(\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=optimizer,\n","    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",")\n","\n","\n","gemma_lm.fit(data, epochs=1, batch_size=1)\n"],"metadata":{"id":"_sxTEI3lW1Ss"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = template.format(\n","    context\n","    instruction=\"\",\n","    response=\"\",\n",")\n","print(gemma_lm.generate(prompt, max_length=256))"],"metadata":{"id":"QbFGTtiiQD_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = 'gemma_lm_model.keras'\n","gemma_lm.save(model_path)"],"metadata":{"id":"tGSyLMIck1LP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","import keras_nlp\n","import tensorflow_text as text\n","\n","\n","model_path = '/content/drive/MyDrive/Gemma/gemma_lm_model.keras'\n","\n","custom_objects = {\n","    'GemmaCausalLM': keras_nlp.models.GemmaCausalLM,\n","    'GemmaCausalLMPreprocessor': keras_nlp.models.GemmaCausalLMPreprocessor,\n","    'GemmaTokenizer': keras_nlp.models.GemmaTokenizer,\n","}\n","\n","gemma_lm = keras.models.load_model(model_path, custom_objects=custom_objects)"],"metadata":{"id":"hg5P25jH1N7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gradio as gr\n","import tensorflow as tf\n","from tensorflow import keras\n","import keras_nlp\n","import tensorflow_text as text\n","\n","\n","model_path = '/content/drive/MyDrive/Gemma/gemma_lm_model.keras'\n","\n","custom_objects = {\n","  'GemmaCausalLM': keras_nlp.models.GemmaCausalLM,\n","  'GemmaCausalLMPreprocessor': keras_nlp.models.GemmaCausalLMPreprocessor,\n","  'GemmaTokenizer': keras_nlp.models.GemmaTokenizer,\n","}\n","\n","gemma_lm = keras.models.load_model(model_path, custom_objects=custom_objects)\n","\n","# Function to generate responses\n","def generate_response(prompt):\n","    formatted_prompt = f\"{prompt}\\nResponse:\"\n","    try:\n","        response = gemma_lm.generate(formatted_prompt, max_length=256)\n","        return response\n","    except Exception as e:\n","        return str(e)\n","\n","# Create Gradio interface\n","interface = gr.Interface(\n","    fn=generate_response,\n","    inputs=gr.Textbox(lines=2, placeholder=\"Enter your instruction here...\"),\n","    outputs=gr.Textbox(lines=10, placeholder=\"Generated response will appear here...\"),\n","    title=\"Gemma Language Model\",\n","    description=\"Generate responses using the Gemma Language Model\"\n",")\n","\n","# Launch the Gradio app\n","interface.launch()\n"],"metadata":{"id":"bzg8qfEFRMLl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install keras-nlp\n"],"metadata":{"id":"Tre7C3y1SB9k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install gradio"],"metadata":{"id":"nnPOTYyiRtaS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"g76jufAfRw8m"},"execution_count":null,"outputs":[]}]}